# 跨主机容器网络

![image-20220702204547649](https://gitee.com/shunyu-online/shunyu_typora_image/raw/master/image/image-20220702204547649.png)

## `libnetwork` & `CNM`

`libnetwork`是`docker`容器网络库，最核心的内容是定义的`Container Network Model(CNM)`，这个模型对容器网络进行了抽象，由以下三类组件组成

- `Sandbox`：`Sanbox`是容器的网络栈，包含容器的`interface`、路由表和`DNS`设置。`Linux Network Namespace`是`Sandbox`的标准实现。`Sandbox`可以包含来自不同`Network`的`Endpoint`。
- `Endpoint`：`Endpoint`的作用是将`Sandbox`接入`Network`。一个`Endpoint`只能属于一个`Network`，也只能属于一个`Sandbox`
- `Network`：`Network`包含一组`Endpoint`，同一个`Network`的`Endpoint`可以直接通信。`network`的实现可以是Linux Bridge、`VLAN`

![image-20220702210149121](https://gitee.com/shunyu-online/shunyu_typora_image/raw/master/image/image-20220702210149121.png)

以`docker bridge driver`为例查看`libnetwork CNM`是怎么实现的

![image-20220702213053254](https://gitee.com/shunyu-online/shunyu_typora_image/raw/master/image/image-20220702213053254.png)

- 两个`network`：默认的`bridge`和自定义网络`my_net2`，实现方式是`Linux Bridge`：`docker0`和`br-5d863e9f78b6`
- 三个`Endpoint`：由`veth pair`实现，一段挂在`Linux Bridge`上，另一端挂在容器内
- 三个`Sandbox`：由`Network Namespace`实现，每个容器有自己的`Sandbox`

## `overlag`

为了支持容器跨主机通信，`docker`提供了`overlag driver`，使用户可以创建基于`VxLAN`的`overlay`网络，`VxLAN`可将二次数据封装到`UDP`进行传输，`VxLAN`提供和`VLAN`相同的以太网二层服务，但是拥有更强的扩展性和灵活性。

`docker overlay`网络需要一个`key-value`数据库用于保存网络状态信息，包括`Network`、`Endpoint`、`IP`等，`Consul`、`Etcd`和`Zookeeper`都是`docker`支持的`key-value`软件

## 搭建实验环境

```shell
# 以容器的方式运行consul
docker run -d -p 8500:8500 -h consul --name consul progrium/consul -server -bootstrap

# 修改host1和host3的docker daemon的配置文件
vi /etc/systemd/system/docker.service.d/10-machine.conf
# 修改为如下内容
root@host3:cat /etc/systemd/system/docker.service.d/10-machine.conf
[Service]
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --storage-driver overlay2 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=generic --cluster-store=consul://10.182.81.24:8500 --cluster-advertise=ens160:2376
Environment=
# -cluster-store=consul://10.182.81.24:8500：指定consul的地址和端口
# --cluster-advertise=ens160:2376：指定自己的网络接口和端口 ens160为接口名称 

# 重启docker daemon
systemctl daemon-reload
systemctl restart docker.service

# host1和host3将自动注册到consul数据库
# 查看10.182.81.24:8500 --> key/value --> docker --> nodes可以看到
```

### 创建 overlay网络

在`host1`中创建`overlay`网络 `ov_net1`

```shell
# -d overlay 指定driver为overlay
root@host3:/etc/systemd/system/docker.service.d# docker network create -d overlay ov_net1
722bc7739c02b888ba5ac1beba0a3181d6dfa841b6ff39cf5acd512701feb017
root@host3:/etc/systemd/system/docker.service.d# docker network ls
NETWORK ID     NAME      DRIVER    SCOPE
e983f7235f5d   bridge    bridge    local
b4ceec7c8e5b   host      host      local
7b5de9c81589   none      null      local
722bc7739c02   ov_net1   overlay   global

# ov_net1的SCOPE为global，而其他网络为local
# 查看另一台设备，发现也存在ov_net1网络
root@host2:~# docker network ls
NETWORK ID     NAME      DRIVER    SCOPE
44220859c817   bridge    bridge    local
ab6b14ee12de   host      host      local
956900a31ca4   none      null      local
722bc7739c02   ov_net1   overlay   global

# 使用docker network inspect ov_net1查看ov_net1的详细信息
root@host2:~# docker network inspect ov_net1 |grep -A 10 IPAM
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "10.0.0.0/24",
                    "Gateway": "10.0.0.1"
                }
            ]
        },
        "Internal": false,
# IPAM是指 IP Address Management, docker自动为ov_net1分配的IP空间为10.0.0.0/24
```

### 在overlay中运行容器

运行一个`busybox`容器，并连接到`ov_net1`，查看容器的网络配置

```shell
root@host2:~# docker run -itd --name bbox1 --network ov_net1 busybox
Unable to find image 'busybox:latest' locally
latest: Pulling from library/busybox
19d511225f94: Pull complete
Digest: sha256:3614ca5eacf0a3a1bcc361c939202a974b4902b9334ff36eb29ffe9011aaad83
Status: Downloaded newer image for busybox:latest
aef9f403bbc61a077956e6a7e2ac35fc650a1ad89b22e925009329cffa6b0276
root@host2:~# docker exec bbox1 ip r
default via 172.18.0.1 dev eth1
10.0.0.0/24 dev eth0 scope link  src 10.0.0.2
172.18.0.0/16 dev eth1 scope link  src 172.18.0.2
```

`bbox1`有两个网络接口`eth0`和`eth1`，`eth0`连接的是`overlay`网络，`eth1`连接的是`docker`创建的一个`docker_gwbridge`，为所有连接到`overlay`网络的容器提供访问外网的能力

```shell
root@host2:~# docker exec -it bbox1 /bin/sh
/ # ping www.baidu.com
PING www.baidu.com (180.101.49.11): 56 data bytes
64 bytes from 180.101.49.11: seq=0 ttl=49 time=11.075 ms
64 bytes from 180.101.49.11: seq=1 ttl=49 time=9.549 ms
```

如果要外网访问容器，可通过主机端口映射

```shell
root@host2:~# docker run -p 80:80 -d --net ov_net1 --name web1 httpd
d7191f70a80203f8735aa8b428c2c07a698970434a5f109b960958562a730b69
```

### overlay网络连通性

在`host3`中运行`bbox2`,查看`bbox2`的`IP`地址为10.0.0.3，并且可以直接`ping bbox1`，可见`overlay`网络中的容器可以直接通信，并且`docker`也实现了`DNS`服务

```shell
docker run -itd --name bbox2 --network ov_net1 busybox
Unable to find image 'busybox:latest' locally
latest: Pulling from library/busybox
5cc84ad355aa: Pull complete
Digest: sha256:5acba83a746c7608ed544dc1533b87c737a0b0fb730301639a0179f9344b1678
Status: Downloaded newer image for busybox:latest
c06afb51c58a78de859b607c637f6dec6b745f95a7fe2649094b99ffb8d32a2f
root@host3:~# docker exec bbox2 ping -c 3 bbox1
PING bbox1 (10.0.0.2): 56 data bytes
64 bytes from 10.0.0.2: seq=0 ttl=64 time=2.174 ms
64 bytes from 10.0.0.2: seq=1 ttl=64 time=0.963 ms
```

#### overlay网络的具体实现

`docker`会为每个`overlay`网络创建一个独立的`network namespace`，其中会有一个`linux bridge br0`，`endpoint`还是由`veth pair`实现，一端连接到容器中，另一端连接到`namespace`的`br0`上。`br0`除了连接所有的`endpoint`，还会连接一个`vxlan`设备，用于与其他`host`建立`vxlan tunnel`，容器之间通过`tunnel`通信

![image-20220706205220286](https://gitee.com/shunyu-online/shunyu_typora_image/raw/master/image/image-20220706205220286.png)

通过`ip netns`查看`overlay`的`namespace`

```shell
# host3
root@host3:~# ln -s /var/run/docker/netns /var/run/netns
root@host3:~# ip netns
0d2933540b8e (id: 1)
1-722bc7739c (id: 0)

# host1
root@host2:~# ln -s /var/run/docker/netns /var/run/netns
root@host2:~# ip netns
b3c6fefb35e7 (id: 2)
45de76cf594b (id: 1)
1-722bc7739c (id: 0)

# 如果报错exec of "brctl" failed: No such file or directory
# 则安装apt-get install bridge-utils
root@host2:~# ip  netns  exec  1-722bc7739c  brctl  show
bridge name     bridge id               STP enabled     interfaces
br0             8000.06ac93b70159       no              veth0
                                                        veth1
                                                        vxlan0

# 查看vxlan0设备的VxLAN ID
root@host2:~# ip  netns  exec 1-722bc7739c  ip  -d  l  show  vxlan0
7: vxlan0@if7: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master br0 state UNKNOWN mode DEFAULT group default
    link/ether 06:ac:93:b7:01:59 brd ff:ff:ff:ff:ff:ff link-netnsid 0 promiscuity 1
    vxlan id 256 srcport 0 0 dstport 4789 proxy l2miss l3miss ttl inherit ageing 300 udpcsum noudp6zerocsumtx noudp6zerocsumrx
    bridge_slave state forwarding priority 32 cost 100 hairpin off guard off root_block off fastleave off learning on flood on port_id 0x8001 port_no 0x1 designated_port 32769 designated_cost 0 designated_bridge 8000.6:ac:93:b7:1:59 designated_root 8000.6:ac:93:b7:1:59 hold_timer    0.00 message_age_timer    0.00 forward_delay_timer    0.00 topology_change_ack 0 config_pending 0 proxy_arp off proxy_arp_wifi off mcast_router 1 mcast_fast_leave off mcast_flood on neigh_suppress off group_fwd_mask 0x0 group_fwd_mask_str 0x0 vlan_tunnel off addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535
```

### overlay网络隔离

不同的overlay网络是相互隔离的，创建第二个overlay网络ov_net2并运行容器bbox3

```shell
root@host2:~# docker network create -d overlay ov_net2
79bd72f05e5d53649126635811adeb64090dfeb00c525b38ccb87f510b62a41a
root@host2:~# docker run -itd --name bbox3 --network ov_net2 busybox
c616823357833d1cdaaa5210fc50efb4c98daacdcfbc38d263ade63d7ada9d1f

# 分别查看bbox1和bbox3的ip
root@host2:~# docker exec bbox1 ip r
default via 172.18.0.1 dev eth1
10.0.0.0/24 dev eth0 scope link  src 10.0.0.2
172.18.0.0/16 dev eth1 scope link  src 172.18.0.2
root@host2:~# docker exec bbox3 ip r
default via 172.18.0.1 dev eth1
10.0.1.0/24 dev eth0 scope link  src 10.0.1.2
172.18.0.0/16 dev eth1 scope link  src 172.18.0.4

# bbox3 ping bbox3的域名、eth0、eth1都不通
root@host2:~# docker exec bbox3 ping -c 2 bbox1
ping: bad address 'bbox1'
root@host2:~# docker exec bbox3 ping -c 2 10.0.0.2
PING 10.0.0.2 (10.0.0.2): 56 data bytes

--- 10.0.0.2 ping statistics ---
2 packets transmitted, 0 packets received, 100% packet loss
root@host2:~# docker exec bbox3 ping -c 2 172.18.0.2
PING 172.18.0.2 (172.18.0.2): 56 data bytes

--- 172.18.0.2 ping statistics ---
2 packets transmitted, 0 packets received, 100% packet loss
root@host2:~#

#　如果要实现不同overlay网络之间的通信，可以使用docker network connect将两个容器连接到同意网络
root@host2:~# docker network connect ov_net1 bbox3
root@host2:~# docker exec bbox3 ping -c 3 bbox1
PING bbox1 (10.0.0.2): 56 data bytes
64 bytes from 10.0.0.2: seq=0 ttl=64 time=3.472 ms
64 bytes from 10.0.0.2: seq=1 ttl=64 time=0.194 ms
64 bytes from 10.0.0.2: seq=2 ttl=64 time=0.171 ms
```

### overlay IPAM

`docker`默认为`overlay`网络分配24位掩码的子网，所有主机共享这个子网，我们也可以使用`--subnet`指定`IP`空间

```shell
root@host2:~# docker network create -d overlay --subnet 192.168.1.0/24 ov_net3
8ea2f7ab2f384ad3c5ad27bb4883b7e8ba81d0f692da5160e94dd72355448adc
root@host2:~# docker network inspect ov_net3 | grep -A 8 "IPAM"
        "IPAM": {
            "Driver": "default",
            "Options": {},
            "Config": [
                {
                    "Subnet": "192.168.1.0/24"
                }
            ]
        },

```

## macvlan

`macvlan`也是一个支持跨主机容器网络的`driver`，也本身是`linux kernel`模块，**他允许同一个物理网卡配置多个mac地址**，即多个`interface`，每个`interface`可以配置自己的`IP`。

`macvlan`本质上是一种网卡虚拟化技术，他最大的优点是性能极好，直接通过`interface`连接到物理网络，不需要创建`Linux bridge`

### 搭建实验环境

为保证多个`mac`地址的网络包都可以从`ens160`通过，需要打开网卡的`promisc`模式

```shell
root@host2:~# ip link set ens160 promisc on
root@host2:~# ip link show ens160
2: ens160: <BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP> mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
    link/ether 00:50:56:9d:64:6f brd ff:ff:ff:ff:ff:ff
root@host2:~# ens160
# 确保包含PROMISC 和 state UP
```

给网卡设置混杂模式，混杂模式就是接收所有经过网卡的数据包，包括不是发给本机的包

```shell
# set: ifconfig eth0 promisc 
# unset: ifconfig eth0 -promisc
```

### 创建macvlan网络

```shell
root@host2:~# docker network create -d macvlan --subnet=192.168.2.0/24   --gateway=192.168.2.1 -o parent=ens160 mac_net1
3efc9809d543340b10f812a1daf91e62ee6491a349e41abf52eb9b71397a3b3f
# -d macvlan 指定driver为macvlan
# macvlan网络是local网络，为了保证能跨主机通信，用户需要自己管理IP子网
# docker 不会为macvlan创建网关，所以macvlan网络的网关应该是真实存在的
# -o parent指定使用网络的interface
# macvlan创建的网络不会通过consul数据库共享，所以要在host1和host3同步执行命令

# 在host1和host3中分别运行容器bbox4，bbox5
# 由于host1和host2中的mac_net1本质上是独立的，为了避免自动分配造成的IP冲突，通过--ip指定bbox4 和 bbox5的ip地址

# host1中
docker run -itd --name bbox4 --ip=192.168.2.4 --network mac_net1 busybox
# host3中
docker run -itd --name bbox5 --ip=192.168.2.5 --network mac_net1 busybox
# 此时bbox4和bbox5是联通的可以相互ping，但是无法ping主机名，所以docker没有为macvlan提供DNS服务
```

### macvlan网络

`macvlan`不依赖`Linux bridge`，`brctl show`可以确认没有创建新的`bridge`

```shell
root@host2:~# brctl show;
bridge name     bridge id               STP enabled     interfaces
docker0         8000.0242e545331d       no
docker_gwbridge         8000.024297643642       no              veth2fdadcb
                                                        vethacb6cdc
                                                        vethdc502e0

# 查看容器的网络设备，除了lo，容器只有一个eth0，容器的eth0就是ens160通过macvlan虚拟出来的interface，容器的interface直接与主机的网卡连接，无须通过NAT和端口映射就可以与外网直接通信
root@host2:~# docker exec bbox4 ip link
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
28: eth0@if2: <BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN> mtu 1500 qdisc noqueue
    link/ether 02:42:c0:a8:02:0a brd ff:ff:ff:ff:ff:ff
```

### 用sub-interface实现多macvlan网络

`macvlan`会独占主机的网卡，一个网卡只能创建一个`macvlan`网络，但是主机网卡的数量是有限的，为了可以支持更多的macvlan网络，可以将macvlan连接到sub-interface

```shell
# 配置sub-interface，在/etc/network/interfaces文件中添加以下内容
auto ens160 iface ens160 inet manual
auto ens160.10 iface ens160.10 inet manual vlan-raw-device ens160
auto ens160.20 iface ens160.20 inet manual vlan-raw-device ens160

# 然后启用sub-interface

```

